
## 핵심 문장

**BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.**

- BERT is designed to pre-train deep bidirectional representations
- 
