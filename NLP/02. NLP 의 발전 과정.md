
# NLP 의 발전 과정

## RNN

대부분의 머신러닝, 딥러닝은 일반적으로 숫자로 이루어진 데이터를 입력과 출력의 형태로 채택한다. 따라서 NLP 에 이러한 기술을 적용하기 위해서는 주어진 text data 를 단어 단위로 분리하고, 각 단어를 특정한 차원으로 이루어진 벡터로 표현하는 과정을 거쳐야 한다. 이를 word 를 vector 공간의 한 점으로 나타낸다는 뜻에서 word embedding 이라고 부른다.

문장은 word vector 들이 특정한 순서로 주어지는 하나의 sequence 로 볼 수 있다. 즉 sequence 는 순서 정보를 포함하기 때문에 같은 단어들이 순서를 달리하면 의미도 다르다는 것을 NLP model 이 인식할 수 있어야 한다.

이런 sequence data 를 처리하는 데 특화된 model 구조로서 RNN 계열 딥러닝 모델이 핵심 모델로 자리잡게 되었다. RNN 계열 모델 중에서 LSTM(Long Short Term Memory)와 이를 단순화 해 계산 속도를 높인 GRU 등이 많이 사용되었다.

## Transformer

구글의 [Attention is all you need](https://arxiv.org/abs/1706.03762) 논문이 발표되면서 RNN 기반의 NLP 모델 구조를 Self-attention 이라 불리는 모듈로 대체하는 Transformer 라는 새로운 모델이 등장했다. NLP 분야에서 transformer 는 큰 성능 향상을 불러왔으며, 현재 대부분의 NLP 를 위한 딥러닝 모델들은 transformer 구조를 채택하고 있다. 현재는 NLP 분야 뿐만 아니라 영상 처리, 시계열 예측, 신약 개발, 신물질 개발 등 다양한 분야에 활발히 적용되고 있다.

Transformer 등장 이전에는 task 마다 특화된 딥러닝 모델이 따로 존재했다. 요즘은 트랜스포머의 핵심 모듈인 self-attention module 을 쌓아 모델 크기를 키우고, 이 모델을 특정 task 를 위한 label 이 필요하지 않은 self-supervised training(자가 지도 학습)을 통해서 대규모의 dataset 으로 모델을 학습시킨다. 그리고 이렇게 사전에 학습된(pre-trained) 모델을 구조의 큰 변화 없이 원하는 특정 task에 transfer learning(전이 학습) 형태로 적용하면 기존의 특화 모델들을 뛰어넘는 성능을 보여준다.

NLP 에서 Self-supervised training(자가 지도 학습)은 입력 문장의 일부 단어를 가리고서 앞뒤 문맥을 보고 그단어를 맞추도록 하는 task 이다. 이러한 task 를 통해 딥러닝 모델이 언어의 문법적$\cdot$의미론적 지식들을 학습할 수 있다.

Self-supervised and pre-trained model 의 예시로 BERT, GPT-2, GPT-3 등이 있다.

그러나 이런 self-supervied and pre-trained model 은 대규의 데이터와 엄청난 양의 GPU resources 를 필요로 한다는 한계점이 있다.