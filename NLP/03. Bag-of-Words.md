# Bag-of-Words

NLP 와 text-mining 분야에서 DL 기술이 적용되기 이전에 많이 활용했던 표현형으로, 단어나 문서를 숫자로 표현해준다. 이를 활용한 문서 분류 기법으로 NaiveBayes Classifier 가 있다.

1. text dataset 에서 unique 한 단어들을 모아서 vocabulary 를 구축한다.
	- Example sentences: "I really really enjoy walking", "She really likes this song"
	- Vocabulary: {"I", "really", "enjoy", "walking", "She", "likes", "this", "song"}
	- 이 단어들은 categorical variables(범주형 변수) 로 볼 수 있다.
2. Encoding unique words to one-hot vectors
	- Vocabulary: {"I", "really", "enjoy", "walking", "She", "likes", "this", "song"}
		- I: $[10000000]$
		- really: $[01000000]$
		- enjoy: $[00100000]$
		- walking: $[00010000]$
		- She: $[00001000]$
		- likes: $[00000100]$
		- this: $[00000010]$
		- song: $[00000001]$
	- 어떤 단어쌍이든지간에 유클리드 거리는 모두 $\sqrt{2}$이다.
	- 어떤 단어쌍이든지간에 내적값 혹은, cosine similarity 는  0이다.
	- 즉 단어의 의미와 상관 없이 모든 단어들이 동일한 관계를 가지도록 단어의 벡터 표현형을 설정한 것이다.
3. 문장이나 문서는 one-hot 벡터의 합으로 표현될 수 있다. 이를 Bag-of-Words vector 라고 부른다.
	- "I really really enjoy walking"
		- $[12110000]$
	- "She really likes this song"
		- $[01001111]$

## NaiveBayes Classifier

문서가 분류될 수 있는 카테고리 혹은 클래스가 $C$ 개 있다고 하자. 예를 들어 문서를 정치, 경제, 문화, 스포츠 중의 하나로 분류할 수 있으면 $C=4$이다. 특정한 문서 $d$ 가 주어져 있을 때, 그 문서가 $C$ 개의 각각의 클래스에 속할 확률 분포는 $P(c|d), \ c \in C$ 이다.

- Map = Maximum a posteriori = most likely class

$$
C_{MAP} = argmax \ P(c|d) = argmax \frac{P(d|c)P(c)}{P(d)} \ (Bayes \ Rule)
$$

$P(d)$는 문서 $d$가 뽑힐 확률인데, $d$는 고정된 하나의 문서이므로 $P(d)$ 는 상수값이고 따라서 $argmax$ operation 에서 무시할 수 있다. 따라서


$$
C_{MAP}=argmax \ P(d|c)P(c)
$$

위와 같은 식이 유도된다. 그리고 $P(d|c)P(c)$ 는 각 단어 $w_1, \cdots, w_n$ 이 given $c$의 조건 하에서 등장할 확률이 서로 독립이라고 가정한다면 즉, $c$ 조건 하에서 서로 독립인 조건부 독립이라고 가정한다면 다음과 같이 표현할 수 있다.

$$
P(d|c)P(c)=P(w_1,w_2,\cdots,w_n|c)P(c) \rightarrow P(c)\Pi_{w_i \in W}P(w_i|c)
$$
