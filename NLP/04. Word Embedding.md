# Word Embedding

- 각 단어들을 특정한 차원으로 이루어진 공간상의 한 점, 혹은 그 좌표를 나타내는 벡터로 나타내는 기술이다.
- 비슷한 의미를 가지는 단어는 좌표 공간 상에 비슷한 위치로 표현되게 한다.
	- 즉, word embedding 은 단어의 의미상의 유사도를 잘 반영한 벡터 표현을 모델에 제공하는 역할을 한다.

## Word2Vec

### Word2Vec 이란

- 같은 문장 안에 있는 인접한 단어들 끼리 의미가 비슷할 것이라고 가정한다.
- 한 단어의 의미를 주변에 등장하는 단어들을 통해 알 수 있다는 것에 착안해, 주어진 학습 데이터를 바탕으로 특정 단어 주변에 나타는 단어들의 확률분포를 예측한다.
	- 단어 "cat" 이 주고 주변에 나타날 단어 $w$ 를 숨긴 채 단어를 예측하도록 한다
	- $P(w|'cat')$

### Word2Vec 알고리즘이 작동하는 방법

주어진 학습 데이터가 "I study math" 인 경우,

1. 주어진 학습데이터를 단어별로 분리하는 tokenization 을 거치고 unique 한 단어를 모아서 사전을 구축한다.
	- Vocabulary: {"I", "study", "math"}
2. 사전의 각 단어는 vocabulary 의 크기만큼의 차원을 가지는 one-hot 벡터로 나타낸다. 따라서 input layer 와 output layer 의 노드 수는 vocabulary 의 크기와 같다.
3. Hidden layer 의 노드 수는 사용자가 정하는 hyper parameter 이다.
	- 만약 입력 노드와 출력 노드가 3차원이고 hidden layer 의 노드 수가 2개이면 $W_1$ 은 $(2\times 3)$ matrix, $W_2$ 는 $(3 \times 2)$ matrix 이다.
	- $softmax(W_2 W_1 x) \cong y$

